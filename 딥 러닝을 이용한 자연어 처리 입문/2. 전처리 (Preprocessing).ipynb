{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "2. 전처리 (Preprocessing).ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLGhbEiOoAR7"
   },
   "source": [
    "# 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E585k45HDx5E"
   },
   "source": [
    "### 1) 토큰화 (Tokenizing)\n",
    "* 텍스트를 자연어 처리를 위해 분리 하는 것을\n",
    "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
    "\n",
    "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "senwNSwgDzQc"
   },
   "source": [
    "### 2) 품사 부착(PoS Tagging)\n",
    "* 각 토큰에 품사 정보를 추가\n",
    "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R15ri5czDyzc"
   },
   "source": [
    "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
    "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
    "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfq99EkzD1Tk"
   },
   "source": [
    "### 4) 원형 복원 (Stemming & Lemmatization)\n",
    "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
    "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
    "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5HQOjRvDxmd"
   },
   "source": [
    "### 5) 불용어 처리 (Stopword)\n",
    "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
    "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaIYJczuaS0n"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysKAL3VlgQN"
   },
   "source": [
    "# 1 영문 전처리 실습\n",
    "\n",
    "\n",
    "NLTK lib (https://www.nltk.org/) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv0ASXb8qa6H"
   },
   "source": [
    "## 1) 영문 토큰화\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZPZeW4nqTpZD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1435f4a9-d51a-4215-9bef-2257f93bf227"
   },
   "source": [
    "!pip install nltk"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.6.7)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ywTmZDer4iH-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eca968e0-6b80-4b4d-8cc6-54ea173ca46d"
   },
   "source": [
    "# word_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Barack Obama likes fried chicken very much'\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rygb4BNXFd13",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "59ae369a-b18c-4d4e-c7cc-23b63da4d7f3"
   },
   "source": [
    "# WordPunctTokenizer() : 알파벳이 아닌문자를 구분하여 토큰화\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = 'Barack Obama likes fried chicken very much'\n",
    "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
    "print(wordpuncttoken)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VrvBRJqJlitx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d26a3af-c8d0-40b2-8738-8f691971dddf"
   },
   "source": [
    "# TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = 'Barack Obama likes fried chicken very much'\n",
    "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
    "print(treebankwordtoken)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Z-0Nnysqnq"
   },
   "source": [
    "## 2) 영문 품사 부착 (PoS Tagging)\n",
    "분리한 토큰마다 품사를 부착한다\n",
    "\n",
    "https://www.nltk.org/api/nltk.tag.html\n",
    "\n",
    "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mHWVrEmTlosg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2625a63d-9ff0-436a-cec1-40bf97deae25"
   },
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jwtt2LxqlrVS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "653fdd9c-dad0-4f0c-ece2-9aa5f734c452"
   },
   "source": [
    "taggedToken = pos_tag(word_tokens)\n",
    "print(taggedToken)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDo-5-khs5Oz"
   },
   "source": [
    "## 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "\n",
    "http://www.nltk.org/api/nltk.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Clj4X6Gilsi9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9efb0fac-01cd-4918-ebb3-b836df504c2f"
   },
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VdkMJHO7mBgi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7bb715d-4f54-43a5-dd81-aae29bf6f877"
   },
   "source": [
    "from nltk import ne_chunk\n",
    "neToken = ne_chunk(taggedToken)\n",
    "print(neToken)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (ORGANIZATION Obama/NNP)\n",
      "  likes/VBZ\n",
      "  fried/VBN\n",
      "  chicken/JJ\n",
      "  very/RB\n",
      "  much/JJ)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHjV0h0ZtM-t"
   },
   "source": [
    "## 4) 원형 복원\n",
    "각 토큰의 원형을 복원하여 표준화 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2eCnbChtXjo"
   },
   "source": [
    "### 4-1) 어간추출 (Stemming)\n",
    "\n",
    "* 규칙에 기반 하여 토큰을 표준화\n",
    "* ning제거, ful 제거 등\n",
    "\n",
    "https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-AvZXHLmCy2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5252937b-0281-4ad1-8fda-99a0e16b3fa6"
   },
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"running -> \" + ps.stem(\"running\"))\n",
    "print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n",
    "print(\"believes -> \" + ps.stem(\"believes\"))\n",
    "print(\"using -> \" + ps.stem(\"using\"))\n",
    "print(\"conversation -> \" + ps.stem(\"conversation\"))\n",
    "print(\"organization -> \" + ps.stem(\"organization\"))\n",
    "print(\"studies -> \" + ps.stem(\"studies\"))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "beautiful -> beauti\n",
      "believes -> believ\n",
      "using -> use\n",
      "conversation -> convers\n",
      "organization -> organ\n",
      "studies -> studi\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4haNWIcCtZza"
   },
   "source": [
    "### 4-2)표제어 추출 (Lemmatization)\n",
    "\n",
    "* 품사정보를 보존하여 토큰을 표준화\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MdxBuzdymR7w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f59cefa7-b3d3-4294-d070-888fa5e7a694"
   },
   "source": [
    "nltk.download('wordnet')"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2mQSzsCZmMBd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b654ef80-a0fc-4c85-a2ab-e87afcd5a04b"
   },
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
    "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
    "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
    "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
    "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
    "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
    "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> running\n",
      "beautiful -> beautiful\n",
      "believes -> belief\n",
      "using -> using\n",
      "conversation -> conversation\n",
      "organization -> organization\n",
      "studies -> study\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmY_SvDMb0fz"
   },
   "source": [
    "## 5) 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lOUE-BBKcn4S"
   },
   "source": [
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CyDJ4JiscnrY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0632618f-108d-4bc0-c7c5-f97c2607661e"
   },
   "source": [
    "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
    "from collections import Counter\n",
    "Counter(taggedToken).most_common()"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('Barack', 'NNP'), 1),\n (('Obama', 'NNP'), 1),\n (('likes', 'VBZ'), 1),\n (('fried', 'VBN'), 1),\n (('chicken', 'JJ'), 1),\n (('very', 'RB'), 1),\n (('much', 'JJ'), 1)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zNhxqDVkcnX9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9c8d0a33-1075-4e7a-e4eb-d02630e8f75c"
   },
   "source": [
    "stopWord = [',','be','able','very']\n",
    "\n",
    "word = []\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "            \n",
    "print(word)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'fried', 'chicken', 'much']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV0orUsOb6wD"
   },
   "source": [
    "## 6) 영문 텍스트 전처리 종합"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pbz6tLP_mNrn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0c21b1ff-821d-4b68-d417-b5ba576f0d06"
   },
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sumtoken = TreebankWordTokenizer().tokenize(\"Obama loves fried chicken of KFC\")\n",
    "print(sumtoken)\n",
    "\n",
    "from nltk import pos_tag\n",
    "sumTaggedToken = pos_tag(sumtoken)\n",
    "print(taggedToken)\n",
    "\n",
    "from nltk import ne_chunk\n",
    "sumNeToken = ne_chunk(sumTaggedToken)\n",
    "print(neToken)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(\"loves -> \" + ps.stem(\"loves\"))\n",
    "print(\"fried -> \" + ps.stem(\"fried\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "print(\"loves -> \" + wl.lemmatize(\"loves\"))\n",
    "print(\"fried -> \" + wl.lemmatize(\"fried\"))\n",
    "\n",
    "#불용어 처리\n",
    "sumStopPos = ['IN']\n",
    "sumStopWord = ['fried']\n",
    "\n",
    "word = []\n",
    "for tag in sumTaggedToken:\n",
    "    if tag[1] not in sumStopPos:\n",
    "        if tag[0] not in sumStopWord:\n",
    "            word.append(tag[0])\n",
    "            \n",
    "print(word)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Obama', 'loves', 'fried', 'chicken', 'of', 'KFC']\n",
      "[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n",
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (ORGANIZATION Obama/NNP)\n",
      "  likes/VBZ\n",
      "  fried/VBN\n",
      "  chicken/JJ\n",
      "  very/RB\n",
      "  much/JJ)\n",
      "loves -> love\n",
      "fried -> fri\n",
      "loves -> love\n",
      "fried -> fried\n",
      "['Obama', 'loves', 'chicken', 'KFC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMErzPcbuYEa"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Dhqm4zkHXl"
   },
   "source": [
    "# 2 한글 전처리 실습\n",
    "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w09FHRgIphw5"
   },
   "source": [
    "## 1) 한글 토큰화 및 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xj3gdRSzhC8n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ae1e50f2-ede4-4195-f143-0f3ac2615a0d"
   },
   "source": [
    "#konlpy 설치\n",
    "!pip install konlpy"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (1.22.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (4.7.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZWN4xX4HXW"
   },
   "source": [
    "한글 자연어처리기 비교\n",
    "\n",
    "https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221337575742"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "__e0d_9Svzor",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a029f849-072e-42e4-828c-386a4a0f9dd7"
   },
   "source": [
    "# 코모란(Komoran) 토큰화\n",
    "from konlpy.tag import Komoran\n",
    "komoran= Komoran()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "komoran_tokens = komoran.morphs(kor_text)\n",
    "print(komoran_tokens)"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Java package 'kr.co.shineware.nlp.komoran.core' is not valid",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 코모란(Komoran) 토큰화\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkonlpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Komoran\n\u001B[1;32m----> 3\u001B[0m komoran\u001B[38;5;241m=\u001B[39m \u001B[43mKomoran\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m kor_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m komoran_tokens \u001B[38;5;241m=\u001B[39m komoran\u001B[38;5;241m.\u001B[39mmorphs(kor_text)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\딥러닝을_이용한_자연어처리-kCGUGv36-py3.9\\lib\\site-packages\\konlpy\\tag\\_komoran.py:69\u001B[0m, in \u001B[0;36mKomoran.__init__\u001B[1;34m(self, jvmpath, userdic, modelpath, max_heap_size)\u001B[0m\n\u001B[0;32m     66\u001B[0m komoranJavaPackage \u001B[38;5;241m=\u001B[39m jpype\u001B[38;5;241m.\u001B[39mJPackage(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkr.co.shineware.nlp.komoran.core\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 69\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjki \u001B[38;5;241m=\u001B[39m \u001B[43mkomoranJavaPackage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mKomoran\u001B[49m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodelpath)\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:  \u001B[38;5;66;03m# Package kr.lucypark.komoran.KomoranInterface is not Callable\u001B[39;00m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot access komoran-dic. Please leave an issue at https://github.com/konlpy/konlpy/issues\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: Java package 'kr.co.shineware.nlp.komoran.core' is not valid"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ZD4PsSCeztM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "62679adf-25d2-46df-c872-fc694ec43a7e"
   },
   "source": [
    "# 한나눔(Hannanum) 토큰화\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum= Hannanum()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "hannanum_tokens = hannanum.morphs(kor_text)\n",
    "print(hannanum_tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ORRFr8tHe1VX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31e29aca-19dd-4a91-a72f-90110df84025"
   },
   "source": [
    "# Okt 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt= Okt()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "okt_tokens = okt.morphs(kor_text)\n",
    "print(okt_tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "COrUs_nHe26J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2ae482db-fc14-4e94-dfb4-8b2a848de2f0"
   },
   "source": [
    "# Kkma 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "kkma= Kkma()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "kkma_tokens = kkma.morphs(kor_text)\n",
    "print(kkma_tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7nyptjunTG"
   },
   "source": [
    "## 2) 한글 품사 부착 (PoS Tagging)\n",
    "\n",
    "PoS Tag 목록\n",
    "\n",
    "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2t6txrctj8nC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "489225d7-83c5-4725-9a2f-eccc607dcbcb"
   },
   "source": [
    "# 코모란(Komoran) 품사 태깅\n",
    "komoranTag = []\n",
    "for token in komoran_tokens:\n",
    "    komoranTag += komoran.pos(token)\n",
    "print(komoranTag)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "msdBCzI6iA2w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "450ff257-c7ba-44c1-b60a-97e02c4eec6c"
   },
   "source": [
    "# 한나눔(Hannanum) 품사 태깅\n",
    "hannanumTag = []\n",
    "for token in hannanum_tokens:\n",
    "    hannanumTag += hannanum.pos(token)\n",
    "print(hannanumTag)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dpe14zC3iCFi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "33d86258-8bee-4218-f7b0-bc0c40fefd73"
   },
   "source": [
    "# Okt 품사 태깅\n",
    "oktTag = []\n",
    "for token in okt_tokens:\n",
    "    oktTag += okt.pos(token)\n",
    "print(oktTag)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xNQBKdYaiDd0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97cb6c4a-52b3-4dd9-a531-caccd4fffb6f"
   },
   "source": [
    "# Kkma 품사 태깅\n",
    "kkmaTag = []\n",
    "for token in kkma_tokens:\n",
    "    kkmaTag += kkma.pos(token)\n",
    "print(kkmaTag)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZY4s8tbuuXP"
   },
   "source": [
    "## 3) 불용어(Stopword) 처리\n",
    "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nvjk1yIYkCfj"
   },
   "source": [
    "#불용어 처리\n",
    "stopPos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "573iqrTFkcJ3"
   },
   "source": [
    "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
    "from collections import Counter\n",
    "#Counter(oktTag).most_common()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5lBkhHm1kYcz"
   },
   "source": [
    "stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BJgERpoikh9s"
   },
   "source": [
    "word = []\n",
    "for tag in oktTag:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iUQTDj4KkkBN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ba9b9a71-13d5-4c3c-cfa7-d4751bf001e1"
   },
   "source": [
    "print(word)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sWmQvENLhenG"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}