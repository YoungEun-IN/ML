## Multi Container PODs

[Extending applications on Kubernetes with multi-container pods (learnk8s.io)](https://learnk8s.io/sidecar-containers-patterns)

컨테이너화된 환경에서 실행되도록 명시적으로 설계되지 않은 애플리케이션을 실행하는 것은 어떻습니까?
*다중 컨테이너 포드를 사용할 수 있습니다.*

포드에서 여러 컨테이너를 실행하려는 이유는 무엇입니까?
*다중 컨테이너 포드를 사용하면 코드를 변경하지 않고도 애플리케이션의 동작을 변경할 수 있습니다.*

### Securing an HTTP service

*다중 컨테이너 포드를 사용하여 개선하려는 예제 애플리케이션으로 Elasticsearch를 사용하겠습니다.*

```yaml
# es-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
    spec:
      containers:
        - name: elasticsearch
          image: elasticsearch:7.9.3
          env:
            - name: discovery.type # The discovery.type environment variable is necessary to get it running with a single replica.
              value: single-node
          ports:
            - name: http
              containerPort: 9200
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
spec:
  selector:
    app.kubernetes.io/name: elasticsearch
  ports:
    - port: 9200
      targetPort: 9200
```

이 포드는 클러스터에서 다른 포드를 실행하고 `elasticsearch` 서비스로 `컬링`하여 작동합니다.

```bash
kubectl run -it --rm --image=curlimages/curl curl \
  -- curl http://elasticsearch:9200
```

```bash
{
  "name" : "elasticsearch-77d857c8cf-mk2dv",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "z98oL-w-SLKJBhh5KVG4kg",
  "version" : {
    "number" : "7.9.3",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "c4138e51121ef06a6404866cddc601906fe5c868",
    "build_date" : "2020-10-16T10:36:16.141335Z",
    "build_snapshot" : false,
    "lucene_version" : "8.6.2",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "we Know, for Search"
}
```

제로 트러스트 보안 모델로 전환하고 있으며 네트워크의 모든 트래픽을 암호화하고 싶다고 가정해 보겠습니다

*응용 프로그램이 기본 TLS를 지원하지 않는 경우 어떻게 해야 합니까?*

초기 아이디어는 수신이 클러스터의 외부 트래픽을 라우팅하는 구성 요소이기 때문에 'nginx 수신'으로 TLS 종료를 수행하는 것입니다. ` pod는 클러스터의 외부 트래픽을 통과할 수 있습니다.

요구 사항을 충족하는 솔루션은 TLS를 수신 대기할 포드에 `nginx` 프록시 컨테이너를 부착하는 것입니다.

```yaml
# es-deployment-v1.1.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
    spec:
      containers:
        - name: elasticsearch
          image: elasticsearch:7.9.3
          env:
            - name: discovery.type
              value: single-node
            - name: network.host
              value: 127.0.0.1
            - name: http.port
              value: '9201'
        - name: nginx-proxy
          image: nginx:1.19.5
          volumeMounts:
            - name: nginx-config
              mountPath: /etc/nginx/conf.d
              readOnly: true
            - name: certs
              mountPath: /certs
              readOnly: true
          ports:
            - name: https
              containerPort: 9200
      volumes:
        - name: nginx-config
          configMap:
            name: elasticsearch-nginx
        - name: certs
          secret:
            secretName: elasticsearch-tls
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-nginx
data:
  elasticsearch.conf: |
    server {
        listen 9200 ssl;
        server_name elasticsearch;
        ssl_certificate /certs/tls.crt;
        ssl_certificate_key /certs/tls.key;

        location / {
            proxy_pass http://localhost:9201;
        }
    }
```

위의 의미는 다음과 같습니다.

- Elasticsearch는 기본 `0.0.0.0:9200`(`network.host` 및 `http.port` 환경 변수의 용도임) 대신 포트 `9201`의 `localhost`에서 수신 대기합니다.
- 새로운 `nginx-proxy` 컨테이너는 HTTPS를 통해 포트 `9200`에서 수신 대기하고 포트 `9201`에서 Elasticsearch에 요청을 프록시합니다. (`elasticsearch-tls` 암호에는 예를 들어 [cert-manager](https://cert-manager.io/)로 생성할 수 있는 TLS 인증서와 키가 포함되어 있습니다.)

클러스터 내에서 HTTPS 요청을 만들어 작동하는지 확인할 수 있습니다.

```bash
kubectl run -it --rm --image=curlimages/curl curl \
  -- curl -k https://elasticsearch:9200
{
  "name" : "elasticsearch-5469857795-nddbn",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "XPW9Z8XGTxa7snoUYzeqgg",
  "version" : {
    "number" : "7.9.3",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "c4138e51121ef06a6404866cddc601906fe5c868",
    "build_date" : "2020-10-16T10:36:16.141335Z",
    "build_snapshot" : false,
    "lucene_version" : "8.6.2",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "we Know, for Search"
}
```

> 자체 서명된 TLS 인증서에는 `-k` 버전이 필요합니다. 프로덕션 환경에서는 신뢰할 수 있는 인증서를 사용하려고 합니다.

#### Types of multi-container patterns

- ##### ***Ambassador Pattern***: *(as Proxy containers)* 

  Ambassador 패턴으로 할 수 있는 몇 가지 다른 작업은 다음과 같습니다.

  - 클러스터의 모든 트래픽을 TLS 인증서로 암호화하려면 클러스터의 모든 포드에 nginx 프록시를 설치하기로 결정할 수 있습니다. 한 단계 더 나아가 '상호 TLS'를 사용하여 모든 요청이 인증되고 암호화되도록 할 수도 있습니다. (*주요 접근 방식은 Istio 및 Linkerd와 같은 서비스 메시를 사용했습니다.*)
  - 프록시를 사용하여 중앙 집중식 OAuth 기관이 JWT를 확인하여 모든 요청을 인증하도록 할 수 있습니다. 이에 대한 한 가지 예는 요청이 GCP Identity-Aware-Proxy에 의해 인증되었는지 확인하는 'gcp-iap-auth'입니다.
  - 보안 터널을 통해 외부 데이터베이스에 연결할 수 있습니다. 이는 기본 제공 TLS 지원이 없는 데이터베이스에 특히 유용합니다. 또 다른 예는 Google Cloud SQL 프록시입니다.

  ```yaml
  # es-ambassador.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: podtest
  spec:
    containers:
      - name: c1
        image: busybox
        command: ['sleep', '5000']
        volumeMounts:
          - name: shared
            mountPath: /shared
      - name: c2
        image: busybox
        command: ['sleep', '5000']
        volumeMounts:
          - name: shared
            mountPath: /shared
    volumes:
      - name: shared
        emptyDir: {}
  ```

  ###### How Ambassador pattern works ?

  - 모든 컨테이너가 동일한 네트워크 네임스페이스를 공유하므로 단일 컨테이너는 외부 연결을 포함한 모든 연결을 수신 대기할 수 있습니다.
  - 나머지 컨테이너는 localhost의 연결만 허용하고 외부 연결은 거부합니다.

  > 네트워크 네임스페이스가 공유되기 때문에 한 포드의 여러 컨테이너가 동일한 포트에서 수신 대기할 수 없다는 점에 유의하세요!

- ##### *어댑터 패턴:* *(표준 인터페이스로 메트릭 노출)*

  Kubernetes 클러스터의 모든 서비스 모니터링에서 Prometheus를 사용하도록 표준화했지만 기본적으로 Prometheus 측정항목을 내보내지 않는 일부 애플리케이션을 사용하고 있다고 가정해 보겠습니다.

  예를 들어 다양한 Elasticsearch 메트릭을 Prometheus 형식으로 노출하는 포드에 `exporter` 컨테이너를 추가해 보겠습니다.

  ```yaml
  # es-prometheus.yml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: elasticsearch
  spec:
    selector:
      matchLabels:
        app.kubernetes.io/name: elasticsearch
    template:
      metadata:
        labels:
          app.kubernetes.io/name: elasticsearch
      spec:
        containers:
          - name: elasticsearch
            image: elasticsearch:7.9.3
            env:
              - name: discovery.type
                value: single-node
            ports:
              - name: http
                containerPort: 9200
          - name: prometheus-exporter
            image: justwatch/elasticsearch_exporter:1.1.0
            args:
              - '--es.uri=http://localhost:9200'
            ports:
              - name: http-prometheus
                containerPort: 9114
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: elasticsearch
  spec:
    selector:
      app.kubernetes.io/name: elasticsearch
    ports:
      - name: http
        port: 9200
        targetPort: http
      - name: http-prometheus
        port: 9114
        targetPort: http-prometheus
  ```

- ##### *사이드카 패턴: (테일링 로그)*

  Elasticsearch 컨테이너가 기본적으로 표준 출력으로 기록하기 때문에 약간 인위적인 Elasticsearch 예제를 다시 가져옵니다.

  ```yaml
  # sidecard-example.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: elasticsearch
    labels:
      app.kubernetes.io/name: elasticsearch
  spec:
    selector:
      matchLabels:
        app.kubernetes.io/name: elasticsearch
    template:
      metadata:
        labels:
          app.kubernetes.io/name: elasticsearch
      spec:
        containers:
          - name: elasticsearch
            image: elasticsearch:7.9.3
            env:
              - name: discovery.type
                value: single-node
              - name: path.logs
                value: /var/log/elasticsearch
            volumeMounts:
              - name: logs
                mountPath: /var/log/elasticsearch
              - name: logging-config
                mountPath: /usr/share/elasticsearch/config/log4j2.properties
                subPath: log4j2.properties
                readOnly: true
            ports:
              - name: http
                containerPort: 9200
          - name: logs
            image: alpine:3.12
            command:
              - tail
              - -f
              - /logs/docker-cluster_server.json
            volumeMounts:
              - name: logs
                mountPath: /logs
                readOnly: true
        volumes:
          - name: logging-config
            configMap:
              name: elasticsearch-logging
          - name: logs
            emptyDir: {}
  ```

  > 로깅 구성 파일은 여기에 포함하기에는 너무 긴 별도의 `ConfigMap`입니다.

  기타 사용 사례:

  - Pod를 다시 시작할 필요 없이 실시간으로 ConfigMap을 다시 로드합니다.
  - Hashicorp Vault의 secret을 애플리케이션에 주입합니다.
  - 지연 시간이 짧은 메모리 내 캐싱을 위해 로컬 Redis 인스턴스를 애플리케이션에 추가합니다.

## initContainers

컨테이너에서 완료될 때까지 실행되는 프로세스를 실행하려는 경우.
예를 들어 기본 웹 애플리케이션에서 사용할 저장소에서 코드 또는 바이너리를 가져오는 프로세스입니다. 이는 Pod가 처음 생성될 때 한 번만 실행되는 작업이거나 실제 애플리케이션이 시작되기 전에 외부 서비스 또는 데이터베이스에서 대기하는 프로세스입니다. 이것이 `initContainers`가 들어오는 곳입니다.

```yaml
# pod-init-containers.yml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    name: myapp
spec:
  containers:
  - name: myapp-core
    image: busybox
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    command: ['sh', '-c', 'echo The app is running ! && sleep 3600']
  initContainers:
    - name: init-mysvc
      image: busybox
      command: ['sh', '-c', 'git clone https://github.com/aditya109/web-crawler-nodejs; done;']
```

포드가 처음 생성되면 'initContainer'가 실행되고 애플리케이션을 호스팅하는 실제 컨테이너가 시작되기 전에 'initContainer'의 프로세스가 완료될 때까지 실행되어야 합니다.

다중 포드 컨테이너에 대해 했던 것처럼 이러한 `initContainers`도 여러 개 구성할 수 있습니다. 이 경우 각 `initContainers`는 **한 번에 하나씩 순차적으로 실행됩니다.**

`initContainer` 중 하나라도 완료되지 않으면 Kubernetes는 `initContainer`가 성공할 때까지 Pod를 반복해서 다시 시작합니다.

`initContainers`는 다음을 제외하면 일반 컨테이너와 똑같습니다.

- `initContainers`는 항상 완료될 때까지 실행됩니다.
- 다음 초기화 컨테이너가 시작되기 전에 각 초기화 컨테이너가 성공적으로 완료되어야 합니다.

Pod의 init 컨테이너가 실패하면 kubelet은 성공할 때까지 init 컨테이너를 반복해서 다시 시작합니다. 그러나 Pod의 'restartPolicy'가 Never이고 해당 Pod를 시작하는 동안 초기화 컨테이너가 실패하면 Kubernetes는 전체 Pod를 실패한 것으로 처리합니다.

포드에 대한 초기화 컨테이너를 지정하려면 앱 `컨테이너` 배열과 함께 `initContainer` 필드를 컨테이너 유형 객체의 배열로 포드 사양에 추가합니다. 초기화 컨테이너의 상태는 컨테이너 상태의 배열로 `.status.initContainerStatuses` 필드에 반환됩니다(`.status.containerStatuses` 필드와 유사).

### Differences from regular containers

`initContainers`는 리소스 제한, 볼륨 및 보안 설정을 포함하여 앱 컨테이너의 모든 필드와 기능을 지원합니다. 그러나 초기화 컨테이너에 대한 리소스 요청 및 제한은 다르게 처리됩니다.

또한 `initContainers`는 포드가 준비되기 전에 실행이 완료되어야 하므로 `lifecycle`, `livenessProbe`, `readinessProbe` 또는 `startupProbe`를 지원하지 않습니다.


포드에 대해 여러 'initContainers'를 지정하면 kubelet은 각 초기화 컨테이너를 순차적으로 실행합니다. 포드가 준비되려면 각 초기화 컨테이너가 성공해야 합니다. 모든 `initContainers` 실행이 완료되면 kubelet은 포드의 애플리케이션 컨테이너를 초기화하고 평소처럼 실행합니다.
