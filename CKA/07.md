## Multiple Schedulers

**custom-scheduler.service**를 만들려면 사용자 지정 스케줄러 바이너리를 사용하고 **–scheduler-name**을 사용자 지정 스케줄러 이름으로 변경합니다.

### Custom Scheduler

```yaml
# my-custom-scheduler.yaml

apiVersion: v1
kind: Pod
metadata: 
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    - --scheduler-name=my-custom-scheduler
    - --lock-object-name=my-custom-scheduler

    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    name: kube-scheduler
```

```yaml
apiVersion: v1 # version of Kubernetes API
kind: Pod # kind of Kubernetes object to be created
metadata: # data about the Kubernetes object `kind`
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    env: test
spec: # specification about Kubernetes object `kind`
  containers:
    - name: nginx-container
      image: nginx
  nodeName: # by default is empty
  ...
  schedulerName: my-custom-scheduler				👈
```

```bash
kubectl get events
kubectl logs my-custom-scheduler --namespace=kube-system
```

## Extras:

1. 이 환경에서 기본 kubernetes 스케줄러를 배포하는 포드의 이름을 얻는 방법은 무엇입니까

   -  ```powershell
     kubectl get pods --namespace=kube-system
     ```

2. 사용자 지정 스케줄러를 만드는 방법은 무엇입니까?

   - 모든 기본 yaml은 `/etc/kubernetes/manifests`에 있습니다.

   - 여기에서 `kube-scheduler.yaml`을 사용자 위치(예: `/root`)에 복사합니다.

   - ```powershell
     mv kube-scheduler.yaml /root/my-scheduler.yaml
     cd /root
     ```

   - Kubernetes 설명서로 이동하여 **다중 스케줄러**를 검색합니다. 포드 정의 섹션을 확인하십시오.

   - ```yaml
     # admin/sched/my-scheduler.yaml 
     apiVersion: v1
     kind: ServiceAccount
     metadata:
       name: my-scheduler
       namespace: kube-system
     ---
     apiVersion: rbac.authorization.k8s.io/v1
     kind: ClusterRoleBinding
     metadata:
       name: my-scheduler-as-kube-scheduler
     subjects:
     - kind: ServiceAccount
       name: my-scheduler
       namespace: kube-system
     roleRef:
       kind: ClusterRole
       name: system:kube-scheduler
       apiGroup: rbac.authorization.k8s.io
     ---
     apiVersion: rbac.authorization.k8s.io/v1
     kind: ClusterRoleBinding
     metadata:
       name: my-scheduler-as-volume-scheduler
     subjects:
     - kind: ServiceAccount
       name: my-scheduler
       namespace: kube-system
     roleRef:
       kind: ClusterRole
       name: system:volume-scheduler
       apiGroup: rbac.authorization.k8s.io
     ---
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       labels:
         component: scheduler
         tier: control-plane
       name: my-scheduler											#👈 name your scheduler - NAME1
       namespace: kube-system						
     spec:
       selector:
         matchLabels:
           component: scheduler
           tier: control-plane
       replicas: 1
       template:
         metadata:
           labels:
             component: scheduler
             tier: control-plane
             version: second
         spec:
           serviceAccountName: my-scheduler
           containers:
           - command:
             - /usr/local/bin/kube-scheduler
             - --address=0.0.0.0
             - --leader-elect=false										#👈 change this to true
             - --scheduler-name=my-scheduler								#👈 change to NAME1
             image: gcr.io/my-gcp-project/my-kube-scheduler:1.0
             livenessProbe:
               httpGet:
                 path: /healthz
                 port: 10251
               initialDelaySeconds: 15
             name: kube-second-scheduler									#👈 changeable as well, but not needed to match NAME1
             readinessProbe:
               httpGet:
                 path: /healthz
                 port: 10251
             resources:
               requests:
                 cpu: '0.1'
             securityContext:
               privileged: false
             volumeMounts: []
           hostNetwork: false
           hostPID: false
           volumes: []
     
     ```

   - ```powershell
     kubectl create -f my-scheduler.yaml 
     ```
  


# Logging and Monitoring 📰

**Kubernetes에서 리소스 소비를 모니터링하는 방법은 무엇입니까? 클러스터에서 무엇을 알고 싶습니까?**

다음과 같은 노드 수준 지표일 수 있습니다.

1. 클러스터의 총 노드 수
2. N클러스터의 정상 노드 수
3. CPU, 메모리, 디스크 사용률.

유사한 팟(Pod) 관련 메트릭도 있을 수 있습니다.

## Monitoring Solution

현재 'Kubernetes'에는 완전한 기능을 갖춘 로깅 및 모니터링 솔루션이 제공되지 않습니다.

그러나 동일한 작업을 수행할 수 있는 다양한 오픈 소스 솔루션이 있습니다.

1. `Metrics Server`(`Heapster` now deprecated)
2. `Prometheus`
3. `Elastic Stack`
4. `Datadog`
5. `dynatrace`

'Metrics Server'는 메모리 내 로깅 서비스이므로 과거 데이터를 볼 수 없습니다.

### How to works?

`kubelet`에는 팟(Pod)의 로그 노출을 담당하는 `cAdvisor`가 포함되어 있습니다.

노드 메트릭을 보려면 `kubectl top node`. 

포드 메트릭을 보기 위한 `kubectl top pod`.

### Kubernetes 리소스에서 라이브 로그를 보는 방법은 무엇입니까?

```yaml
# event-simulator.yaml
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
```

```powershell
🐳 » kubectl create -f event-simulator.yml
pod/event-simulator-pod created
🐳 » kubectl logs -f event-simulator-pod
[2021-01-25 15:28:30,805] INFO in event-simulator: USER3 logged out
[2021-01-25 15:28:31,806] INFO in event-simulator: USER4 logged out
[2021-01-25 15:28:32,807] INFO in event-simulator: USER4 is viewing page1
[2021-01-25 15:28:33,809] INFO in event-simulator: USER3 is viewing page1
[2021-01-25 15:28:34,810] INFO in event-simulator: USER1 is viewing page3
[2021-01-25 15:28:35,812] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2021-01-25 15:28:35,812] INFO in event-simulator: USER1 logged out
[2021-01-25 15:28:36,813] INFO in event-simulator: USER2 logged in
[2021-01-25 15:28:37,815] INFO in event-simulator: USER1 is viewing page2
[2021-01-25 15:28:38,816] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
[2021-01-25 15:28:38,816] INFO in event-simulator: USER4 is viewing page1
[2021-01-25 15:28:39,818] INFO in event-simulator: USER1 is viewing page3
[2021-01-25 15:28:40,819] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
```

이러한 로그는 팟(Pod) 내부에서 실행되는 컨테이너에 고유합니다.
하지만 `event-simulator.yaml`을 변경하여 추가 포드를 실행한다고 가정해 보겠습니다. 

```yaml
# event-simulator.yaml
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
  - name: image-processor
    image: some-image-processor
```

하지만 `kubectl logs -f event-simulator-pod`에 의해 로그가 표시되는 이 리소스 `event-simulator.yaml`을 다시 실행하면 어떻게 될까요?

여기에서 해당 컨테이너 이름을 지정해야 하며 관련 컨테이너 로그가 표시됩니다.

```bash
kubectl logs -f event-simulator-pod event-simulator
```

# Application Lifecycle Management

## Rolling Updates and Rollbacks in Deployments

### Rollout and Versioning

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/rollouts.svg)

클러스터에서 실행 중인 여러 애플리케이션 집합이 있고 애플리케이션의 *버전* 'Revision 1'에 있다고 가정해 보겠습니다. 이제 실행 중인 모든 애플리케이션을 'Revision 2'로 업그레이드해야 합니다. 실행 중인 모든 애플리케이션을 필요한 버전으로 업그레이드하는 프로세스를 **출시**라고 합니다.

```powershell
kubectl rollout status deployment/myapp-deployment
```

The above command shows the status of the pushed rollout.

```powershell
kubectl rollout history deployment/myapp-deployment
```

위의 명령은 배포의 개정 및 기록을 보여줍니다.

### Deployment Strategy

> 설정: 각각 애플리케이션을 실행하는 4개의 실행 중인 포드가 있습니다.

배포에는 널리 사용되는 2가지 전략이 있습니다.

- 애플리케이션의 인스턴스 4개를 모두 중단한 다음 새 인스턴스 4개를 가져와 교체합니다. 이를 **재생성** 전략이라고 합니다.
  ![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/st1.svg)
  이 전략의 문제는 업데이트 프로세스에서 문제가 발생하면 기존 사용자를 수용할 실행 중인 인스턴스가 없고 새 인스턴스도 없다는 것입니다(문제가 발생했거나 업데이트 프로세스 중에). 실제로 **BFB**(Bad for Business)인 케이터링 사용자를 위한 실행 인스턴스가 없습니다.

- 애플리케이션의 대체 인스턴스를 중단하고 남은 인스턴스는 이전 버전에서 계속 실행한 다음 중단된 인스턴스를 대체할 새 인스턴스를 가져옵니다. 이를 **롤링 업데이트** 전략이라고 합니다.
  ![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/st2.svg)

### 업데이트를 적용하는 방법 ??

이것이 애플리케이션 배포 yaml 매니페스트라는 예를 들어 보겠습니다. 이것은 개정 1 매니페스트입니다.

```yaml
# deployment-definition.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  selector:
    matchLabels:
      app: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp-pod
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx						
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
  replicas: 3
```

업데이트하려면 이미지 이름을 `nginx`에서 `nginx:1.7.1`로 업데이트하면 됩니다. 이것은 개정 2 매니페스트입니다.

```yaml
# deployment-definition.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  selector:
    matchLabels:
      app: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp-pod
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.1					👈
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
  replicas: 3
```

이 업그레이드를 적용하려면 다음을 입력합니다.

```powershell
kubectl apply -f deployment-definition.yml
```

이렇게 하면 새 롤아웃이 트리거되고 새 버전이 생성됩니다.

또한 빠른 팁으로 이미지를 업데이트하기 위해 다음을 수행할 수도 있습니다.

```powershell
kubectl set image deployment/myapp-deployment nginx=nginx:1.7.1
```

> 'RollingUpdate'는 업그레이드를 위한 K8s 기본값입니다.

### Rollback

변경을 취소하려면 다음을 입력하십시오.

```powershell
kubectl rollout undo deployment/myapp-deployment
```

> 'RollingUpdate'는 업그레이드를 위한 K8s 기본값입니다.
  
