## Switching and Routing

2개의 시스템 A와 B가 있다고 가정해 보겠습니다. 서로 어떻게 통신할까요?

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-1.svg)

**스위치**에 연결합니다.
![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-2.svg)

각 시스템을 스위치에 연결하려면 일종의 인터페이스를 만들어야 합니다. 이를 위해 두 시스템 모두에서 다음 명령을 사용합니다.

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-3.svg)

```bash
ip link # to view the switch interface name

ip addr add 192.168.1.10/24 dev eth0 # assign the system A to networking interface

ip addr add 192.168.1.11/24 dev eth0 # assign the system B to networking interface
```

> 현재 스위치는 노드 A와 B 사이의 통신만 가능하며 외부 트래픽 입/출력은 허용되지 않습니다.

서로 다른 네트워크에 있는 호스트 B를 C에 연결하는 방법은 무엇입니까?

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-4.svg)

이를 위해서는 **라우터**가 필요합니다. 라우터에는 각 스위치 포트에 하나씩 2개의 IP가 할당됩니다. 이렇게 하면 두 스위치 간의 통신이 가능해집니다.

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-5.svg)

그러나 B는 어떻게 C에 데이터를 보낼까요?

이를 위해서는 **게이트웨이**도 필요합니다. 외부 네트워크와 통신하는 방법입니다.

기존 라우팅 테이블 구성을 보려면 다음을 사용합니다.

```bash
route
```

B에 다른 호스트 C를 추가하려면 다음을 사용합니다.

```bash
ip route add 192.168.2.0/24 via 192.168.1.1
```

> 모든 대상 호스트에 대해 이 작업을 수행해야 합니다.

호스트를 인터넷 `172.217.194.0/24`에 연결하려면 라우터를 인터넷에 연결해야 합니다. 그런 다음 새 라우팅 구성을 호스트에 추가합니다.

```bash
ip route add 172.217.194.0/24 via 192.168.2.1
```

인터넷에서 사용할 수 있는 라우팅 사이트가 너무 많습니다. 동일한 이름에 여러 라우팅 테이블 항목을 추가하는 대신 다음을 사용할 수 있습니다.

```bash
$ ip route add default via 192.168.2.1
# or
$ ip route add 0.0.0.0 via 192.168.2.1
```

이제 현재 네트워크 외부의 모든 요청은 라우터를 통과합니다.

네트워크에 하나의 라우터만 있을 때 이것은 괜찮습니다. 그러나 여러 라우터가 있을 때 여러 라우팅 구성이 있어야 합니다.

**다음 시나리오**: A를 C에 어떻게 연결하시겠습니까?

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/networking-scenario-2.svg)

A에서 IP<sub>C</sub>를 핑하면;

```bash
ping 192.168.2.5
# connect: network is unreachable
```

따라서 A에서는 라우팅 테이블에 라우팅 구성이 필요합니다.

```bash
ip route add 192.168.2.0/24 via 192.168.1.6
```

C에서는 라우팅 테이블에 라우팅 구성이 필요합니다.

```bash
ip route add 192.168.1.0/24 via 192.168.2.6
```

이제 우리는 A에서 IP<sub>C</sub>를 ping할 수 있지만 C에서 응답을 받지 못합니다. 이유는 기본적으로 Linux가 데이터 패킷 전달을 허용하지 않기 때문입니다. 이를 활성화하려면 호스트 C에서 `/proc/sys/net/ipv4/ip_forward` 파일의 값을 0에서 1로 덮어씁니다.

```bash
echo 1 > /proc/sys/net/ipv4/ip_forward
1
cat /proc/sys/net/ipv4/ip_forward
1
```

> 이 변경 사항은 재부팅 후에도 지속되지 않으므로 `/etc/sysctl.conf` 파일에서 수정해야 합니다.
>
> ```bash
> ...
> net.ipv4.ip_forward = 1
> ...
> ```

## DNS

![](https://raw.githubusercontent.com/aditya109/learning-k8s/a5821e04abcc0d86569ad9b827b130a1df3dc7aa/assets/networking-dns.svg)

이 설정을 고려하십시오. 우리가 하고자 하는 것은 IP 대신 호스트 B를 주어진 이름 `db`를 사용하여 A에서 호스트 B로 핑하는 것입니다.

```bash
cat >> /etc/hosts
192.168.1.11 db
```

이제 `/etc/hosts`는 호스트 A에 대한 진실의 소스입니다. `/etc/hosts` 파일에서 무엇을 보든 그것이 진실이라고 생각하고 그에 따라 요청을 보내기 시작합니다.

여기서 문제는 `/etc/hosts` 파일의 항목이 항상 사실이 아닐 수 있다는 것입니다. 예를 들어 호스트 B에서 `hostname` 명령을 실행하면 호스트 B의 실제 이름이 `host-2`임을 알 수 있습니다.

그러나 호스트 A는 이 사실을 모르기 때문에 `db` 호스트에 대한 요청이 오면 모든 요청을 호스트 B로 다시 라우팅합니다.

이를 이름 확인이라고 합니다.

하지만 이 프로세스는 확장할 수 없습니다. 예를 들어 100대의 서버가 있고 그 중 최소 3대의 서버가 IP를 업데이트했다고 가정해 보겠습니다.

먼저 100개 서버 모두에 대해 `/etc/hosts` 파일을 수동으로 업데이트하고 일부 호스트 IP가 업데이트되면 모든 서버에 대해 수동으로 업데이트해야 한다고 게시해야 합니다. 이 모든 항목은 이제 DNS 서버라는 특수 서버로 이동됩니다.

호스트에 DNS 서버를 추가하려면 `/etc/resolv.conf` 파일에 항목을 추가하기만 하면 됩니다.

```bash
cat /etc/resolv.conf
nameserver   192.168.1.100

ping db
```

이제 모든 IP-호스트 이름 항목이 호스트-IP 변경과 함께 한 곳에서만 업데이트됩니다. 따라서 `/etc/hosts`에 대한 필요성이 제거되어 개인용으로 계속 사용할 수 있습니다.

그러나 DNS 서버와 `/etc/hosts/` 파일에 2개의 유사한 항목이 있으면 어떻게 됩니까?

```bash
# DNS server entries
139.183.121.188   web
49.236.74.206   db
189.197.163.121   nfs
6.184.138.211   db-1
4.10.176.191   nfs-3
237.149.65.17   db-7
172.159.243.60   web-19
56.119.78.108   test   👈
72.149.228.30   nfs-prod
27.48.22.70    sql
```

```bash
# /etc/hosts
192.168.1.115   test   👈
```

여기에서 먼저 `/etc/hosts`를 본 다음 DNS 항목을 봅니다.

이제 호스트 A에서 `netflix.com`을 ping하려면 어떻게 해야 합니까?

Google DNS에 대한 새 항목을 로컬 DNS 서버에 추가할 수 있습니다.

```bash
# DNS server entries
139.183.121.188   web
49.236.74.206   db
189.197.163.121   nfs
6.184.138.211   db-1
4.10.176.191   nfs-3
237.149.65.17   db-7
172.159.243.60   web-19
56.119.78.108   test   👈
72.149.228.30   nfs-prod
27.48.22.70    sql

Forward All to 8.8.8.8
```

다음 레코드가 있는 호스트 A에 연결된 조직 DNS가 있다고 가정해 보겠습니다.

```bash
# DNS server(192.168.1.100) entries
192.168.1.10   web.mycompany.com
192.168.1.11   db.mycompany.com
192.168.1.12   nfs.mycompany.com
192.168.1.13   web-1.mycompany.com
192.168.1.14   sql.mycompany.com
192.168.1.15   hr.mycompany.com
```

이제 'web'을 누를 때마다 트래픽을 'web.mycompany.com'으로 다시 라우팅해야 합니다.

이를 위해 `/etc/resolv.conf`에 항목을 추가해야 합니다.

```bash
cat /etc/resolv.conf
nameserver   192.168.1.100
search    mycompany.com
```

여기서 검색 항목은 모든 검색 항목에 `mycompany.com`을 추가한 다음 DNS에서 검색합니다.

또한 '검색' 필드의 항목은 누적된 항목 중 하나를 DNS에서 검색할 수 있음을 의미합니다. 예를 들어,

```bash
cat /etc/resolv.conf
nameserver   192.168.1.100
search    mycompany.com prod.mycompany.com
```

검색 가능한 항목은 다음과 같습니다.

1. web.mycompany.com
2. web.prod.mycompany.com

### Record Types

| A     | web-server      | 192.168.1.1                       |
| ----- | --------------- | --------------------------------- |
| AAAA  | web-server      | fe80::e496:dacf:ecc7:62e2%9       |
| CNAME | food.web-server | eat.web-server, hungry.web-server |

### nslookup

위의 명령을 사용하여 개별 호스트에 대해 다른 호스트를 ping할 수 있습니다.

> 여기서 `/etc/hosts` 파일의 개별 호스트 항목은 이름 확인에 고려되지 않고 DNS만 고려됩니다.

```bash
nslookup www.google.com
Server:  dsldevice.lan
Address:  192.168.1.1

Non-authoritative answer:
Name:    www.google.com
Addresses:  2404:6800:4002:821::2004
          142.250.194.100
```

### dig

위의 명령은 더 많은 정보가 있는 개별 호스트에 대해 다른 호스트를 ping하는 데 사용할 수도 있습니다.

> 여기서 `/etc/hosts` 파일의 개별 호스트 항목은 이름 확인에 고려되지 않고 DNS만 고려됩니다.

```bash
dig www.google.com
```

## CoreDNS

### 전용 시스템을 DNS로 구성 - CoreDNS

1. curl 또는 wget을 사용하여 바이너리를 다운로드합니다. 그리고 그것을 추출하십시오. coreDNS 실행 파일을 얻습니다.

   ```bash
   wget https://github.com/coredns/coredns/releases/download/v1.8.4/coredns_1.8.4_linux_arm64.tgz
   ```

   ```bash
   tar -xzvf coredns_1.8.4_linux_arm64.tgz
   ```

   ```bash
   ./coredns
   ```

2. 실행 파일을 실행하여 DNS 서버를 시작합니다. (기본 포트 53)

3. 일부 구성을 제공해야 하는 IP-호스트 이름 매핑을 지정하지 않았습니다.

   1. 이를 위해 모든 항목을 DNS 서버 `/etc/hosts/` 파일에 넣습니다.

4. 그런 다음 해당 파일을 사용하도록 CoreDNS를 구성합니다. CoreDNS는 `Corefile`이라는 파일에서 구성을 로드합니다.
   다음은 `/etc/hosts` 파일에서 호스트 이름 매핑에 대한 IP를 가져오도록 CoreDNS에 지시하는 간단한 구성입니다. DNS 서버가 실행되면 이제 서버의 `/etc/hosts/` 파일에서 IP와 이름을 선택합니다.

## Network Namespaces

컨테이너가 생성되면 이를 위한 네트워크 네임스페이스를 생성합니다. 자체 네트워크 네임스페이스 내에서 컨테이너는 자체 가상 라우팅 인터페이스, 라우팅 테이블 및 ARP 테이블을 가질 수 있습니다.

### Creating network namespaces

2개의 네트워크 네임스페이스 `red` 및 `blue`를 생성해 보겠습니다.

```shell
ip netns add red
ip netns add blue
```

시스템에 있는 모든 네트워크 네임스페이스를 보려면

```bash
ip netns
```

### Exec in network namespaces

모든 인터페이스를 보려면

```shell
ip link
```

특정 네트워크 인터페이스에서 인터페이스를 보려면 다음을 사용합니다.

```shell
ip netns exec red ip link
# or
ip -n red link
```

> 그래서 여기에서 컨테이너가 호스트 네트워크 인터페이스를 피칭하는 것을 방지했습니다.

특정 IP 주소에 대한 ARP 테이블을 표시합니다. 또한 ARP 캐시 또는 테이블의 모든 항목을 표시합니다.

```shell
arp
```

특정 네트워크 인터페이스의 ARP 항목에 다음을 사용합니다.

```shell
ip netns exec red arp
```

라우팅 테이블 항목을 표시하려면 다음을 사용합니다.

```shell
route
# for specific network
ip netns exec red route
```

네트워크 네임스페이스에는 기본적으로 네트워크 연결이 없습니다. 그리고 기본 호스트 네트워크를 볼 수 없습니다.

### Connecting network interfaces

네트워크 인터페이스에 함께 연결하려면 가상 케이블 또는 **파이프**가 필요합니다. (인터페이스가 2개인 가상 케이블)

먼저 `veth-red` 및 `veth-blue`라는 끝 인터페이스가 있는 파이프를 만듭니다.

```shell
ip link add veth-red type veth peer name veth-blue
```

이제 파이프의 최종 인터페이스를 해당 네트워크 네임스페이스에 연결해야 합니다.

```shell
ip link set veth-red netns red
ip link set veth-blue netns blue
```

이제 각각의 **pipe-end-interface**-**network-namespace** 쌍에 IP 주소를 할당해야 합니다.

```shell
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-red
```

이제 인터페이스를 불러와야 합니다.

```shell
ip -n red link set veth-red up
ip -n blue link set veth-blue up
```

이제 red 네임스페이스에서 IP<sub>blue</sub>를 ping할 수 있습니다.

```shell
ip netns exec red ping 192.168.15.2
```

각 네임스페이스 ARP 테이블에서도 해당 항목을 볼 수 있습니다.

```shell
ip netns exec red arp
ip netns exec blue arp
```

> `blue` 또는 `red` 네임스페이스와 관련하여 유사한 항목을 전혀 표시하지 않는 `arp`.

### 상호 통신을 위한 여러 네임스페이스 활성화

![](https://raw.githubusercontent.com/aditya109/learning-k8s//main/assets/multi-interface-inter-connection.png)

이를 위해 모든 네임스페이스를 가상 스위치에 연결하면서 가상 네트워크와 가상 스위치를 생성합니다.

먼저 `v-net-0`이라는 `Linux Bridge`를 사용하여 브리지를 생성합니다. 호스트의 관점에서 보면 인터페이스와 정확히 유사합니다.

```shell
ip link add v-net-0 type bridge
```

`ip link` 명령을 사용하여 볼 수 있습니다.

기본적으로 DOWN이므로 UP으로 설정해야 합니다. 이를 위해 다음을 사용합니다.

```shell
ip link set dev v-net-0 up
```

> To delete a virtual cable, we use:
>
> ```shell
> ip -n red link del veth-red
> ```

> 여기서 주의할 점은 하나의 pipe-end-interface를 삭제하면 다른 하나가 즉시 삭제된다는 것입니다.

새로운 파이프가 필요합니다.

```shell
ip link add veth-red type veth peer name veth-red-br
ip link add veth-blue type veth peer name veth-blue-br
```

이제 브리지와 함께 새 파이프에 네임스페이스를 연결합니다.

```shell
ip link set veth-red netns red
ip link set veth-red-br master v-net-0

ip link set veth-blue netns blue
ip link set veth-blue-br master v-net-0
```

이제 IP 주소를 네트워크 네임스페이스에 연결합니다.

```shell
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.1 dev veth-blue
```

이제 각 인터페이스를 'up'으로 가져옵니다.

```shell
ip -n red link set veth-red up
ip -n blue link set veth-blue up
```

### 호스트 시스템과 브리지 간의 통신 활성화

![](https://github.com/aditya109/learning-k8s/blob/main/assets/host-interface-communication.jpg?raw=true)

이를 위해 `v-net-0` 브리지에 IP 주소를 연결합니다.

```shell
ip addr add 192.168.15.5/24 dev v-net-0
```

Ping `192.168.15.5` from hostname.

```shell
ping 192.168.15.5
```

### 외부 LAN 연결에 Linux Bridge 연결 활성화 - 192.168.1.0

`blue` 네임스페이스에서 이 외부 IP `192.168.1.3`(외부 라우터 `192.168.1.0`에 연결된)에 직접 ping을 시도하면 어떻게 되는지 살펴보겠습니다.

```shell
# 먼저 기존 연결에 대한 라우팅 테이블을 확인합니다.
ip netns exec blue route

# 외부 IP 192.168.1.3을 직접 핑하는 해당 IP에 대한 관련 항목이 없기 때문입니다.
ip netns exec blue ping 192.168.1.3
Connect: Network is unreachable
```

![](https://github.com/aditya109/learning-k8s/blob/main/assets/external-network-connection-to-namespace.jpg?raw=true)

**Solution:** 외부 라우터 '192.168.1.0'을 통해 외부 IP '192.168.1.3'을 ping해야 합니다.

```shell
ip netns exec blue ip router add 192.168.1.0/24 via 192.168.15.5

# 이제 외부 IP 192.168.1.3을 ping하면
ip netns exec blue ping 192.168.1.3
PING 192.168.1.3 (192.168.1.3) 56(84) bytes of data.

# 그러나 외부 라우터에 호스트 IP가 없기 때문에 응답을 받지 못하는 것을 볼 수 있습니다. 이를 위해 여기 게이트웨이에서 NAT를 활성화해야 합니다.
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE

# 이제 우리가 외부 세계를 핑할 때
ip netns exec blue ping 192.168.1.3
PING 192.168.1.3 (192.168.1.3) 56(84) bytes of data.
64 bytes from 192.168.1.3: icmp_seq=1 ttl=63 time=0.587 ms
64 bytes from 192.168.1.3: icmp_seq=1 ttl=63 time=0.466 ms
```

### 네트워크 네임스페이스에서 인터넷으로의 연결

```shell
ip netns exec blue ping 8.8.8.8
Connect: Network is unreachable

# 이유는 라우팅 테이블 항목이 누락되었기 때문입니다.
ip netns exec blue route

# 기본 라우팅 항목 추가
ip netns exec blue ip route add default via 192.168.15.5

# 지금 핑하면 작동합니다
ip netns exec blue ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=0.587 ms
64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=0.466 ms
```

### 인터넷에서 네트워크 네임스페이스로의 연결

```shell
ping 192.168.15.2
Connect: Network is unreachable
```

한 가지 방법은 호스트 IP를 통해 네트워크 네임스페이스에 도달할 수 있지만 라우팅 설정을 제공하므로 안전하지 않다는 것을 알려주는 것입니다.

두 번째 방법은 IPTABLE 라우팅 구성을 설정하는 것입니다.

```shell
iptables -t nat -a PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT
```

> **Notes:**
>
> 1. 네트워크 네임스페이스를 테스트하는 동안 한 네임스페이스에서 다른 네임스페이스를 ping할 수 없는 문제가 발생하면 IP 주소를 설정하는 동안 NETMASK를 설정해야 합니다. 즉: `192.168.1.10/24`
>
>    ```shell
>    ip -n red addr add 192.168.1.10/24 dev veth-red
>    ```
>
> 2. 확인해야 할 또 다른 사항은 방화벽 D/IP 테이블 규칙입니다. 하나의 네임스페이스에서 다른 네임스페이스로의 트래픽을 허용하도록 IP 테이블에 규칙을 추가하십시오. 또는 IP 테이블을 모두 비활성화합니다(학습 환경에서만).

## Docker Networking

Docker가 설치된 단일 서버를 가정해 보겠습니다. 로컬 네트워크에 연결된 IP가 '192.168.1.10'인 'eth0' 인터페이스를 가지고 있습니다.

컨테이너를 생성하는 동안 다음 네트워크 옵션을 사용할 수 있습니다.

1. `docker run --network none nginx`
   여기서 도커 컨테이너는 어떤 네트워크에도 연결되어 있지 않습니다. 들어오고 나가는 연결이 없습니다. 이것은 완전한 _네트워크 격리_입니다.

2. `docker run --network host nginx`

여기서 도커 컨테이너는 포트 **80**의 호스트 네트워크에 연결됩니다. 즉, 포트 80에서 웹 애플리케이션을 실행하는 컨테이너는 URI `https://192.168.1.10:80`에서 실행될 수 있습니다.

   > 이러한 컨테이너의 인스턴스는 하나만 실행할 수 있으며 하나의 애플리케이션만 동시에 포트를 사용할 수 있습니다.

3. `docker run nginx`

여기서 도커 컨테이너는 각 컨테이너가 고유한 개인 IP를 갖는 브리지 네트워크에 연결됩니다.

   Docker는 이 네트워크를 `docker network ls`의 출력에 표시되는 이름인 `bridge`로 생성하지만 호스트에서 생성된 네트워크 이름은 `docker0`(`ip link`의 출력에 표시됨)입니다. ).

   따라서 `docker0`과 `bridge`는 하나이며 동일한 것입니다.

   `docker0`에는 `ip addr` 명령으로 볼 수 있는 IP가 부여됩니다. `ip link`는 `docker0` 인터페이스가 기본적으로 다운되었음을 알려줍니다. Docker는 또한 이에 대한 네트워크 네임스페이스를 생성합니다. `ip netns`를 사용하여 볼 수 있습니다.

**Docker는 컨테이너를 브리지 네트워크에 어떻게 연결합니까?**

컨테이너가 생성될 때마다 Docker는 컨테이너에 대한 네트워크 네임스페이스를 생성합니다. Docker가 생성한 네임스페이스는 `ip netns` 명령(약간 수정된 버전)을 사용하여 볼 수 있습니다.
`ip netns exec "${container_id}" ip -s link show eth0`

![](https://github.com/aditya109/learning-k8s/blob/main/assets/docker-networking.png?raw=true)

Docker는 가상 케이블을 생성하고 한쪽 끝을 Docker의 브리지 네트워크에 연결하고 다른 쪽 끝은 컨테이너 자체에 연결합니다.

여기서 다음 명령어를 사용하면,

```bash
ip netns
b3165c10a92b
```

그런 다음 `ip link`를 사용하면 출력에 `vethbb1c343@if7`이 표시됩니다.

또한 가상 케이블의 다른 쪽 끝은 동일한 'ip link' 명령으로 볼 수 있습니다.

```bash
ip -n b3165c10a92b link
eth0@if8
```

컨테이너에는 `ip addr`로 볼 수 있는 별도의 IP가 할당됩니다.

```bash
ip -n b3165c10a92b addr
```

동일한 프로세스가 여러 번 반복됩니다.

브리지 네트워크에 연결된 상태에서 컨테이너가 웹 애플리케이션을 호스팅하는 경우 포트 **80**에서 도커에 액세스할 수 있지만 호스트에서는 액세스할 수 없습니다.

이를 위해서는 포트를 게시해야 합니다.

`docker run --publish HOST_PORT:CONTAINER_PORT <container-name>`

**Docker의 IP 규칙을 보는 방법 ?**

```shell
iptables -nvL -t nat
Chain Docker (2 references)
```

| target | prot | opt | source   | destination |                                |
| ------ | ---- | --- | -------- | ----------- | ------------------------------ |
| RETURN | all  | --  | anywhere | anywhere    |                                |
| DNAT   | tcp  | --  | anywhere | anywhere    | tcp dpt:8080 to :172.17.0.2:80 |

## CNI

**_Container Runtime_**

1. 네트워크 네임스페이스 생성

**_Bridge_** `bridge add <cid> <namespaceid>`

2. Create Bridge Network/Interface
3. Create VETH Pairs (pipes)
4. Attach pipes to Namespaces
5. Attach pipes to Bridge
6. Assign IP addresses
7. Bring the interfaces up
8. Enable NAT -IP Masquerade

### 컨테이너 런타임용 CNI 지시어

- Container Runtime은 네트워크 네임스페이스를 생성해야 합니다.
- 컨테이너가 연결해야 하는 네트워크를 식별합니다.
- 컨테이너 추가 시 네트워크 플러그인(브리지)을 호출하는 컨테이너 런타임.
- 컨테이너가 삭제될 때 네트워크 플러그인(브리지)을 호출하는 컨테이너 런타임.
- 네트워크 구성의 JSON 형식.

Examples, Kubernetes, rkt, Mesos, etc.

> Docker가 CNI를 따르지 않는 이유는 무엇입니까?
>
> 컨테이너 네트워크 모델(CNM)이라는 자체 네트워킹 인터페이스 표준이 있습니다.

### 네트워크 플러그인에 대한 CNI 지시어

- 명령줄 인수 ADD/DEL/CHECK를 지원해야 합니다.
- 매개변수 컨테이너 ID, 네트워크 ns 등을 지원해야 합니다.
- POD에 대한 IP 주소 할당을 관리해야 합니다.
- 특정 형식으로 결과를 반환해야 합니다.

Examples, Bridge, VLAN, IPVLAN, MACVLAN, WINDOWS, DHCP, host-local.

Also, `weaveworks`, `flannel`, `cilium`, `vmwareNSX`, etc.

## Cluster Networking

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/cluster-networking.svg)

클러스터에 대한 몇 가지 전제 조건이 있습니다.

1. 각 노드에는 IP, 노드 이름 및 MAC 주소가 있어야 합니다.
2. 일부 포트는 노드에서도 열려 있어야 합니다.

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/mandatory-port-openings-on-node.png)

![](https://raw.githubusercontent.com/aditya109/learning-k8s/main/assets/mandatory-port-openings-on-master-node.png)

**Master node(s)-**

| Protocol | Direction | Port Range | Purpose                 | Used by              |
| -------- | --------- | ---------- | ----------------------- | -------------------- |
| TCP      | Inbound   | 6443\*     | Kubernetes API server   | All                  |
| TCP      | Inbound   | 2379-2380  | etcd server client API  | kube-apiserver, etcd |
| TCP      | Inbound   | 10250      | Kubelet API             | Self, control plane  |
| TCP      | Inbound   | 10251      | kube-scheduler          | Self                 |
| TCP      | Inbound   | 10252      | kube-controller manager | Self                 |

**Worker node(s)-**

| Protocol | Direction | Port Range  | Purpose               | Used by             |
| -------- | --------- | ----------- | --------------------- | ------------------- |
| TCP      | Inbound   | 10250       | Kubernetes API        | Self, control plane |
| TCP      | Inbound   | 30000-32767 | NodePort Services\*\* | All                 |

**클러스터의 네트워킹 문제 디버깅을 위한 편리한 명령**

```shell
ip link
```

```shell
ip addr
```

```shell
ip addr add 192.168.1.10/24 dev eth0
```

```shell
ip route
```

```shell
ip route add 192.168.1.10/24 via 192.168.2.1
```

```shell
cat /proc/sys/net/ipv4/ip_forward
1
```

```shell
arp
```

```shell
netstat -plnt
```

**마스터 노드에서 클러스터 연결을 위해 구성된 네트워크 인터페이스는 무엇입니까?**

```shell
kubectl get nodes -o wide | grep controlplane

controlplane   Ready    control-plane,master   9m47s   v1.20.0   10.27.140.6   <none>        Ubuntu 18.04.5 LTS   5.4.0-1052-gcp   docker://19.3.0

# copy internal ip from there

ip a | grep -B2 10.27.140.6
7180: eth0@if7181: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default
    link/ether 02:42:0a:1b:8c:06 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.27.140.6/24 brd 10.27.140.255 scope global eth0

# eth0 is the network interface
```

**node01의 MAC 주소는 무엇입니까?**

```shell
arp node01
Address                  HWtype  HWaddress           Flags Mask            Iface
10.27.140.8              ether   02:42:0a:1b:8c:07   C                     eth0
```

**기본 게이트웨이의 IP 주소는 무엇입니까?**

```shell
ip route show default
default via 172.17.0.1 dev eth1
```

**ETCD는 두 개의 포트에서 수신 대기 중입니다. 다음 중 더 많은 클라이언트 연결이 설정되어 있는 것은 무엇입니까?**

```shell
netstat -anp | grep etcd | grep PORT_NUMBER | wc -l
```

## Kubernetes 네트워킹 모델을 구현하는 방법

[Creating Highly Available clusters with kubeadm | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node)
