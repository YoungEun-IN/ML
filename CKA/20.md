## Choosing Kubernetes Infrastructure

로컬 설치의 경우:

- Minikube - VM 배포, 단일 노드 클러스터 생성
- `kubeadm` - 준비된 VM, 단일/다중 노드 클러스터 필요

두 가지 유형의 솔루션:

- 턴키 솔루션 - (VM 프로비저닝, VM 구성, 클러스터 배포용 스크립트, VM 유지 관리 필요). 예: KOPS를 사용하는 AWS의 Kubernetes
- 호스팅 솔루션 - (관리형 솔루션) 예: GKE

## Configure High Availability

여러 개의 중복 스케줄러 및 기타 컨트롤 플레인 구성 요소를 제공하는 여러 마스터 노드가 있습니다.

### How does it work ?

2개의 마스터 노드가 있다고 가정해 보겠습니다. 또 다른 Kubernetes 객체 `kube-controller-manager` 엔드포인트가 있습니다.

여러 마스터 노드를 갖기 위해 `kube-controller-manager`를 정의하는 동안 몇 가지 옵션을 제공합니다.

```sh
kube-controller-manager --leader-elect true
						--leader-elect-lease-duration 15s
						--leader-elect-renew-deadline 10s
						--leader-elect-retry-period 2s
```

## ETCD in HA

### 일관성을 어떻게 유지합니까?

3개의 ETCD 복제본이 실행 중이고 일부 데이터를 작성하려고 한다고 가정해 보겠습니다. 일관성을 보장하는 방법은 무엇입니까?

모든 복제본이 데이터 쓰기를 담당하는 것은 아닙니다. 인스턴스 중 1개는 리더로 선택되고 나머지는 팔로워가 됩니다. 리더만 쓰기를 처리하고 다른 복제본에 동일한 데이터가 기록되도록 합니다.

*대부분의 복제본에서 데이터 복제가 발생하면 쓰기가 완료된 것으로 간주됩니다.*

#### How is leader elected ?

ETCD 리더 선출 프로토콜을 RAFT라고 합니다. 무작위 타이머는 ETCD의 3개 복제본 모두에서 시작됩니다.

![](https://github.com/aditya109/learning-k8s/blob/main/assets/etcd-1.svg?raw=true)

인스턴스가 먼저 시간 초과되면 리더 역할을 맡도록 다른 두 인스턴스에 요청을 보냅니다.

![](https://github.com/aditya109/learning-k8s/blob/main/assets/etcd-2.svg?raw=true)

요청이 승인되면 요청 인스턴스가 리더로 간주됩니다.

![](https://github.com/aditya109/learning-k8s/blob/main/assets/etcd-3.svg?raw=true)

여기에서 알 수 있듯이 'R2'가 리더 역할을 합니다. 또한 'R2'는 계속해서 리더임을 다른 노드에 지속적으로 알립니다.
어떤 이유로 'R2'가 동일한 것을 보내지 않으면 나머지 복제본, 즉 'R1'과 'R3' 사이에서 선택 프로세스가 반복됩니다.

#### 쓰기는 어떻게 전파됩니까?

그러나 팔로워 2명 중 1명이 데이터 복제에 실패했거나 복제본 자체가 다운된 경우에는 어떻게 됩니까?

> *대부분의 복제본에서 데이터 복제가 발생하면 쓰기가 완료된 것으로 간주됩니다.*

이는 리더를 포함하여 3개 노드 중 2개 노드에서 성공적으로 작성했음을 의미합니다. 즉, 쓰기가 성공한 것으로 간주됩니다.

실패한 복제본이 온라인 상태가 되면 해당 복제본에서도 데이터가 복제됩니다.

```
Majority/Quorum = floor(N/2 + 1); N = 클러스터의 총 노드 수
```

| Instances N                                                  | Quorum Q | Fault Tolerance *f=N-Q* |
| ------------------------------------------------------------ | -------- | ----------------------- |
| 1                                                            | 1        | 0                       |
| 2                                                            | 2        | 0                       |
| [👉](https://emojipedia.org/backhand-index-pointing-right/) 3 | 2        | 1                       |
| 4                                                            | 3        | 1                       |
| [👉](https://emojipedia.org/backhand-index-pointing-right/) 5 | 3        | 2                       |
| 6                                                            | 4        | 2                       |
| [👉](https://emojipedia.org/backhand-index-pointing-right/) 7 | 4        | 3                       |
|                                                              |          |                         |

> 네트워크 파티션의 경우 내결함성이 항상 유지되므로 항상 홀수 클러스터 설치 쌍을 선택하십시오.

# Install Kubernetes Cluster-the kubeadm-way

1. 주어진 리포지토리 https://github.com/kodekloudhub/certified-kubernetes-administrator-course에서 `Vagrantfile`을 사용하여 1개의 마스터, 2개의 작업자-클러스터 설정을 가져옵니다.

   ```sh
   vagrant up
   ```

   설정이 완료되면 터미널에서 `vagrant status`를 실행하면 다음과 같이 표시됩니다.

   ```sh
   Current machine states:
   
   kubemaster                running (virtualbox)
   kubenode01                running (virtualbox)
   kubenode02                running (virtualbox)
   
   이 환경은 여러 VM을 나타냅니다. VM은 모두 현재 상태와 함께 위에 나열됩니다. 특정 VM에 대한 자세한 내용을 보려면 `vagrant status NAME`을 실행하십시오.
   ```

2. 가져온 VM에 별도의 세션에 로그인합니다.

   ```sh
   vagrant ssh kubemaster
   ```

   ```sh
   vagrant ssh kubenode01
   ```

   ```sh
   vagrant ssh kubenode02
   ```

3. **모든 노드에서 다음 명령을 실행합니다.**

   ```sh
   sudo modprobe br_netfilter && lsmod | grep br_netfilter
   ```

   This should give you the following output:

   ```sh
   br_netfilter           24576  0
   bridge                155648  1 br_netfilter
   ```

   Further,

   ```sh
   sudo -i
   # iptables가 연결된 트래픽을 볼 수 있도록 허용
   cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
   br_netfilter
   EOF
   
   cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   EOF
   sudo sysctl --system
   
   ## installing container-runtime {DOCKER}
   # removing old versions (if any)
   sudo apt-get remove docker docker-engine docker.io containerd runc
   # setting up apt repository
   sudo apt-get update 
   sudo apt-get install \
       ca-certificates \
       curl \
       gnupg \
       lsb-release 
   curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
   echo \
     "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \
     $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   # installing docker runtime
   sudo apt-get update
   sudo apt-get install docker-ce docker-ce-cli containerd.io
   # configuring docker daemon to use systemd for the management of the container's cgroups
   sudo mkdir /etc/docker
   cat <<EOF | sudo tee /etc/docker/daemon.json
   {
     "exec-opts": ["native.cgroupdriver=systemd"],
     "log-driver": "json-file",
     "log-opts": {
       "max-size": "100m"
     },
     "storage-driver": "overlay2"
   }
   EOF
   # restart and enable on boot
   sudo systemctl enable docker
   sudo systemctl daemon-reload
   sudo systemctl restart docker
   
   ## installing kubeadm, kubectl and kubelet
   sudo apt-get update
   sudo apt-get install -y apt-transport-https ca-certificates curl
   sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
   echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   
   # 도커를 설치하기 때문에 cgroup 드라이버가 필요하지 않습니다.
   ```

4. 마스터 노드에서 실행

   ```sh
   kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.56.2
   ```

   > `--control-plane-endpoint`는 네트워킹 솔루션 CIDR 외부에 있어야 합니다.
   > `--apiserver-advertise-address`의 IP는 `enp0s8`입니다. (`ifconfig enp0s8`을 사용하고 `inet` 필드 값을 가져옵니다.)

   명령이 완료되면 다음과 같은 결과가 출력되어야 합니다.

   ```sh
   Your Kubernetes control-plane has initialized successfully!
   
   클러스터 사용을 시작하려면 일반 사용자로 다음을 실행해야 합니다.
   
     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   또는 루트 사용자인 경우 다음을 실행할 수 있습니다.
   
     export KUBECONFIG=/etc/kubernetes/admin.conf
   
   이제 포드 네트워크를 클러스터에 배포해야 합니다.
   다음에 나열된 옵션 중 하나를 사용하여 "kubectl apply -f [podnetwork].yaml"을 실행합니다.
     https://kubernetes.io/docs/concepts/cluster-administration/addons/
   
   그런 다음 각각에서 루트로 다음을 실행하여 여러 작업자 노드에 가입할 수 있습니다.
   
   kubeadm join 192.168.56.2:6443 --token oiaauw.qb1l8dgaknwk4b8j \
   	--discovery-token-ca-cert-hash sha256:1ce828ac52d733c603d3d265174bc1317bfafef51ec92f5b85e26300de7f3b60 
   ```

   **따라서 마스터에서** `sudo` 사용자 액세스에서 `logout`하고 일반 사용자로 다음을 실행하십시오.

   ```sh
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   ```

   이제 네트워킹 솔루션 `Weave net` **을 마스터**에 설치합니다.

   ```sh
   kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
   ```

   **이제 작업자 노드에서** 다음을 실행합니다.

   ```sh
   kubeadm join 192.168.56.2:6443 --token oiaauw.qb1l8dgaknwk4b8j --discovery-token-ca-cert-hash sha256:1ce828ac52d733c603d3d265174bc1317bfafef51ec92f5b85e26300de7f3b60 
   ```

   **이제 마스터 노드에서** 다음을 실행합니다.

   ```sh
   watch kubectl get nodes
   # wait for all the nodes to come online
   ```
